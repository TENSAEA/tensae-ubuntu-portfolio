<div align="center">
  <img src="https://github.com/tensaeaschalew/deep-learning-assignment/blob/main/assets/animation.gif" width="60px" />
  
  <br/>
  
  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&weight=500&size=28&pause=1000&color=F75C7E&center=true&vCenter=true&width=700&lines=Tensae+Aschalew's+Deep+Learning+Assignment+Report;MNIST+Neural+Network+Analysis;Activation+Functions+%26+Network+Depth" alt="Typing SVG" />
</div>

---

### 📘 Overview

> This report presents the findings from a **Deep Learning Assignment** that explores core aspects of neural network design using the **MNIST dataset**.

- Implementation using Python & PyTorch
- Focused on **activation functions** and **network depth**
- Includes **visualizations**, **performance analysis**, and **recommendations**

---

### 🔍 Key Insights

| Topic | Summary |
|------|---------|
| 🚀 **Activation Functions** | **ReLU** solves the vanishing gradient problem and performs better than Sigmoid/Tanh in both speed and accuracy. |
| 🧠 **Network Depth** | Deeper networks offer more representation power. For MNIST, **4–8 layers** gives the best trade-off between accuracy and computational cost. |

---

### 🧪 Experiments

- Designed multiple neural networks with varying depths
- Compared **ReLU**, **Sigmoid**, and **Tanh** functions
- Tracked performance metrics: accuracy, loss, convergence rate
- Visualized training history and final predictions

---

### 🗂 Report Structure

```plaintext
├── Activation Functions
│   └── ReLU vs Sigmoid vs Tanh
├── Depth of Neural Networks
│   └── Shallow vs Deep models
└── Summary of Key Insights
